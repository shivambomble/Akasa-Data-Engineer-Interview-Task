# ğŸ“‹ Akasa Air ETL Pipeline - Technical Documentation

## ğŸ¯ Project Overview

This document provides comprehensive technical documentation for the Akasa Air ETL (Extract, Transform, Load) Pipeline with integrated analytics dashboard. The solution processes customer and order data from multiple sources (CSV, XML) and provides real-time business intelligence through an interactive web dashboard.

---

## ğŸ“Š Business Problem & Solution

### **Problem Statement**
Akasa Air needed an automated system to:
- Process customer data from CSV files
- Process order data from XML files  
- Validate and clean data for quality assurance
- Store processed data in MySQL database
- Provide business insights through analytics

### **Solution Delivered**
A production-ready ETL pipeline with:
- **Automated data processing** from multiple file formats
- **Comprehensive data validation** and quality checks
- **Scalable database operations** with transaction safety
- **Interactive analytics dashboard** for business intelligence
- **Enterprise-grade architecture** with proper error handling

---

## ğŸ—ï¸ System Architecture

### **High-Level Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚â”€â”€â”€â–¶â”‚   ETL Pipeline   â”‚â”€â”€â”€â–¶â”‚   MySQL DB      â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ CSV Files     â”‚    â”‚ â€¢ Extract        â”‚    â”‚ â€¢ customers     â”‚
â”‚ â€¢ XML Files     â”‚    â”‚ â€¢ Transform      â”‚    â”‚ â€¢ orders        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Load           â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                â”‚                        â”‚
                                â–¼                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                       â”‚   Dashboard      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚   Launcher       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Streamlit      â”‚
                       â”‚   Dashboard      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Component Architecture**
```
main.py
â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_cleaners.py
â”‚   â”‚   â”œâ”€â”€ CustomerCleaner
â”‚   â”‚   â””â”€â”€ OrderCleaner
â”‚   â”œâ”€â”€ database_loader.py
â”‚   â””â”€â”€ dashboard_launcher.py
â””â”€â”€ dashboard_app.py
```

---

## ğŸ”§ Technical Implementation

### **1. Configuration Management (`config.py`)**

**Purpose**: Centralized configuration and database connection management

**Key Features**:
- Environment variable validation
- Database connection pooling
- Secure credential handling
- Configuration error detection

**Implementation Details**:
```python
class Config:
    def __init__(self):
        load_dotenv()  # Load .env file
        self.mysql_user = os.getenv("MYSQL_USER")
        # ... other credentials
        self._validate_config()  # Ensure all required vars exist
    
    def get_database_engine(self):
        # Creates SQLAlchemy engine with connection pooling
        return create_engine(connection_string)
```

**Environment Variables Required**:
```env
MYSQL_USER=your_username
MYSQL_PASSWORD=your_password  
MYSQL_DB=your_database
MYSQL_HOST=your_host
```

### **2. Data Processing (`data_cleaners.py`)**

#### **CustomerCleaner Class**

**Purpose**: Process and validate customer data from CSV files

**Data Validation Rules**:
- **Mobile Number**: Must match Indian format `^[789]\d{9}$`
- **Duplicate Detection**: Based on customer_id and mobile_number
- **Missing Data**: Remove rows with null critical fields
- **Standardization**: Region names converted to Title case
- **Column Mapping**: Rename columns for database compatibility

**Processing Flow**:
```python
def clean_customers(csv_file):
    1. Load CSV file â†’ pandas DataFrame
    2. Strip whitespace from all string columns
    3. Remove duplicates (customer_id + mobile_number)
    4. Validate mobile number format
    5. Remove rows with missing critical data
    6. Standardize region names (Title case)
    7. Rename columns for database schema
    8. Return cleaned DataFrame
```

**Data Transformations**:
```
Input:  customer_id, customer_name, mobile_number, region
Output: customerid, customername, mobilenumber, region
```

#### **OrderCleaner Class**

**Purpose**: Process and validate order data from XML files

**Data Validation Rules**:
- **Order ID Format**: Must match `^ORD-\d{4}-\d+$` pattern
- **Mobile Number**: Must match Indian format `^[789]\d{9}$`
- **Business Rules**: Positive amounts and quantities only
- **DateTime Parsing**: Convert to MySQL-compatible format
- **Data Types**: Proper type conversion with error handling

**Processing Flow**:
```python
def clean_orders(xml_file):
    1. Parse XML file â†’ ElementTree
    2. Extract order elements
    3. For each order:
       - Extract and validate order_id format
       - Validate mobile_number format  
       - Parse and format datetime
       - Validate business rules (positive values)
       - Convert data types
    4. Create DataFrame from valid records
    5. Return cleaned DataFrame
```

**XML Structure Handled**:
```xml
<orders>
  <order>
    <order_id>ORD-2025-0001</order_id>
    <mobile_number>9123456781</mobile_number>
    <order_date_time>2025-10-12T09:15:32</order_date_time>
    <sku_id>SKU-1001</sku_id>
    <sku_count>2</sku_count>
    <total_amount>7450</total_amount>
  </order>
</orders>
```

### **3. Database Operations (`database_loader.py`)**

**Purpose**: Handle all database interactions with optimization and safety

**Key Features**:
- **Batch Processing**: Efficient bulk insertions using `method='multi'`
- **Transaction Safety**: Automatic rollback on errors
- **Connection Management**: Proper resource cleanup
- **Error Handling**: Comprehensive error logging and propagation

**Implementation**:
```python
class DatabaseLoader:
    def load_to_mysql(self, df, table_name):
        try:
            df.to_sql(
                table_name, 
                con=self.engine, 
                if_exists='append',  # Append to existing data
                index=False,         # Don't include DataFrame index
                method='multi'       # Batch insertion for performance
            )
        except Exception as e:
            logging.error(f"Failed to load data into {table_name}: {e}")
            raise  # Re-raise for upstream handling
```

**Database Schema**:
```sql
-- customers table
CREATE TABLE customers (
    customerid VARCHAR(50),
    customername VARCHAR(255),
    mobilenumber VARCHAR(15),
    region VARCHAR(100)
);

-- orders table  
CREATE TABLE orders (
    orderid VARCHAR(50),
    mobilenumber VARCHAR(15),
    orderdatetime DATETIME,
    skuid VARCHAR(50),
    skucount INT,
    totalamount DECIMAL(10,2)
);
```

### **4. Pipeline Orchestration (`pipeline.py`)**

**Purpose**: Coordinate the entire ETL workflow

**Workflow Steps**:
```python
def run(self, customers_csv, orders_xml, launch_dashboard=False):
    1. Initialize components (cleaners, loader, launcher)
    2. Process customer data:
       - Extract from CSV
       - Clean and validate
       - Load to database
    3. Process order data:
       - Extract from XML
       - Clean and validate  
       - Load to database
    4. Optional: Launch analytics dashboard
    5. Cleanup resources (database connections)
```

**Error Handling Strategy**:
- **Component-level**: Each component handles its own errors
- **Pipeline-level**: Catches and logs all errors with context
- **Resource Cleanup**: Ensures database connections are properly closed
- **Graceful Degradation**: Continues processing when possible

### **5. Dashboard Application (`dashboard_app.py`)**

**Purpose**: Provide interactive business intelligence and analytics

#### **Dashboard Architecture**
```python
class DashboardApp:
    def __init__(self):
        self.config = Config()           # Database configuration
        self.setup_page_config()         # Streamlit settings
        self.setup_custom_css()          # Professional styling
    
    def run(self):
        1. Establish database connection
        2. Create region filter interface
        3. Execute analytical queries
        4. Render KPI cards
        5. Display meaningful visualizations
        6. Show detailed data tables
```

#### **Analytics Queries**

**1. Repeat Customers Analysis**:
```sql
SELECT c.customerid, c.customername, COUNT(DISTINCT o.orderid) AS num_orders
FROM customers c 
JOIN orders o ON c.mobilenumber = o.mobilenumber
GROUP BY c.customerid, c.customername
HAVING COUNT(DISTINCT o.orderid) > 1
```

**2. Monthly Business Trends**:
```sql
SELECT 
    DATE_FORMAT(orderdatetime, '%Y-%m') AS month,
    COUNT(DISTINCT o.orderid) AS total_orders,
    SUM(o.totalamount) AS total_revenue,
    AVG(o.totalamount) AS avg_order_value,
    SUM(o.skucount) AS total_items
FROM customers c 
JOIN orders o ON c.mobilenumber = o.mobilenumber
GROUP BY month 
ORDER BY month
```

**3. Regional Performance**:
```sql
SELECT c.region, 
    SUM(o.totalamount) AS total_revenue,
    COUNT(DISTINCT o.orderid) AS total_orders,
    COUNT(DISTINCT c.customerid) AS unique_customers
FROM customers c 
JOIN orders o ON c.mobilenumber = o.mobilenumber
GROUP BY c.region 
ORDER BY total_revenue DESC
```

**4. Order Value Distribution**:
```sql
SELECT 
    CASE 
        WHEN o.totalamount < 1000 THEN 'Under â‚¹1,000'
        WHEN o.totalamount < 5000 THEN 'â‚¹1,000 - â‚¹5,000'
        WHEN o.totalamount < 10000 THEN 'â‚¹5,000 - â‚¹10,000'
        ELSE 'Above â‚¹10,000'
    END AS order_range,
    COUNT(DISTINCT o.orderid) AS order_count,
    SUM(o.totalamount) AS total_revenue
FROM customers c 
JOIN orders o ON c.mobilenumber = o.mobilenumber
GROUP BY order_range
```

#### **Visualization Strategy**

**Meaningful Charts Only**:
- **Monthly Trends**: Line chart showing orders and revenue over time (only if multiple months)
- **Regional Distribution**: Pie chart showing revenue share by region (only if multiple regions)

**Enhanced Data Tables**:
- **Calculated Metrics**: Growth rates, percentages, averages
- **Formatted Display**: Currency formatting, proper number formatting
- **Business Insights**: Key metrics highlighted as text

**Interactive Features**:
- **Regional Filtering**: Dynamic filtering across all visualizations
- **Tabbed Views**: Organized data presentation
- **Responsive Design**: Mobile-friendly interface

### **6. Dashboard Management (`dashboard_launcher.py`)**

**Purpose**: Handle Streamlit dashboard lifecycle and deployment

**Key Features**:
- **Dependency Management**: Automatic installation of required packages
- **Process Management**: Background process control
- **Browser Integration**: Automatic browser opening
- **Port Management**: Configurable port settings

**Implementation**:
```python
class DashboardLauncher:
    def launch_dashboard(self, auto_open=True):
        1. Check if Streamlit is installed
        2. Install missing dependencies if needed
        3. Validate dashboard file exists
        4. Launch Streamlit process
        5. Open browser (if requested)
        6. Return process handle for management
```

### **7. Command Line Interface (`main.py`)**

**Purpose**: Provide flexible execution options and user interface

**Command Options**:
```bash
# ETL only
python main.py --customers data.csv --orders data.xml

# ETL + Dashboard
python main.py --customers data.csv --orders data.xml --dashboard

# Dashboard only
python main.py --dashboard-only
```

**Argument Processing**:
```python
def parse_arguments():
    parser = argparse.ArgumentParser(description="Akasa Air ETL Pipeline")
    parser.add_argument("--customers", help="Path to customers CSV file")
    parser.add_argument("--orders", help="Path to orders XML file")
    parser.add_argument("--dashboard", action="store_true", 
                       help="Launch dashboard after ETL")
    parser.add_argument("--dashboard-only", action="store_true",
                       help="Launch only dashboard")
    return parser.parse_args()
```

---

## ğŸ“Š Data Flow & Processing

### **Complete Data Flow**
```
1. INPUT SOURCES
   â”œâ”€â”€ customers.csv (Customer data)
   â””â”€â”€ orders.xml (Order transactions)
   
2. EXTRACTION PHASE
   â”œâ”€â”€ CSV Reader â†’ pandas DataFrame
   â””â”€â”€ XML Parser â†’ ElementTree â†’ DataFrame
   
3. TRANSFORMATION PHASE
   â”œâ”€â”€ Data Validation
   â”‚   â”œâ”€â”€ Format validation (mobile numbers, order IDs)
   â”‚   â”œâ”€â”€ Business rule validation (positive amounts)
   â”‚   â””â”€â”€ Data type conversion
   â”œâ”€â”€ Data Cleaning
   â”‚   â”œâ”€â”€ Duplicate removal
   â”‚   â”œâ”€â”€ Missing data handling
   â”‚   â””â”€â”€ Standardization (region names)
   â””â”€â”€ Data Mapping
       â””â”€â”€ Column renaming for database schema
       
4. LOADING PHASE
   â”œâ”€â”€ Database Connection (SQLAlchemy)
   â”œâ”€â”€ Batch Insertion (method='multi')
   â””â”€â”€ Transaction Management
   
5. ANALYTICS PHASE
   â”œâ”€â”€ SQL Queries (Business Intelligence)
   â”œâ”€â”€ Data Aggregation
   â””â”€â”€ Visualization Rendering
```

### **Error Handling Flow**
```
Error Occurs
â”œâ”€â”€ Component Level
â”‚   â”œâ”€â”€ Log error with context
â”‚   â”œâ”€â”€ Attempt recovery if possible
â”‚   â””â”€â”€ Raise exception if critical
â”œâ”€â”€ Pipeline Level  
â”‚   â”œâ”€â”€ Catch component exceptions
â”‚   â”œâ”€â”€ Log pipeline context
â”‚   â””â”€â”€ Decide on continuation vs termination
â””â”€â”€ Application Level
    â”œâ”€â”€ Log application-level errors
    â”œâ”€â”€ Cleanup resources
    â””â”€â”€ Exit with appropriate code
```

---

## ğŸ”’ Security & Best Practices

### **Security Measures**

**1. Credential Management**:
- Environment variables for all sensitive data
- No hardcoded credentials in source code
- `.env` file excluded from version control

**2. SQL Injection Prevention**:
- Parameterized queries throughout
- SQLAlchemy ORM for safe database operations
- Input validation before database operations

**3. Input Validation**:
- Format validation for all input data
- Business rule validation
- Data type checking and conversion

**4. Error Information**:
- Sanitized error messages (no sensitive data exposure)
- Structured logging with appropriate levels
- Error context without credential leakage

### **Code Quality Standards**

**1. Documentation**:
- Comprehensive docstrings for all functions/classes
- Type hints for better code clarity
- Inline comments for complex logic

**2. Error Handling**:
- Try-catch blocks around all risky operations
- Specific exception handling
- Proper resource cleanup (database connections)

**3. Modularity**:
- Single Responsibility Principle
- Loose coupling between components
- High cohesion within modules

**4. Performance**:
- Batch database operations
- Connection pooling
- Memory-efficient data processing

---

## ğŸ“ˆ Performance Considerations

### **Database Performance**

**1. Batch Operations**:
```python
# Efficient batch insertion
df.to_sql(table_name, con=engine, method='multi')
```

**2. Connection Pooling**:
```python
# SQLAlchemy handles connection pooling automatically
engine = create_engine(connection_string)
```

**3. Transaction Management**:
- Automatic transaction handling by pandas/SQLAlchemy
- Rollback on errors to maintain data integrity

### **Memory Management**

**1. Streaming Processing**:
- Process data in chunks for large files
- Immediate cleanup of temporary objects

**2. DataFrame Optimization**:
- Efficient data type usage
- Memory-conscious operations

### **Dashboard Performance**

**1. Query Optimization**:
- Efficient SQL queries with proper JOINs
- Indexed columns for better performance
- Aggregation at database level

**2. Caching Strategy**:
- Streamlit's built-in caching for expensive operations
- Connection reuse within dashboard session

---

## ğŸ§ª Testing Strategy

### **Unit Testing Approach**

**1. Component Testing**:
```python
# Example test structure
def test_customer_cleaner():
    # Test data validation
    # Test duplicate removal
    # Test column mapping
    
def test_order_cleaner():
    # Test XML parsing
    # Test format validation
    # Test business rules
    
def test_database_loader():
    # Test connection handling
    # Test batch operations
    # Test error scenarios
```

**2. Integration Testing**:
- End-to-end pipeline testing
- Database integration testing
- Dashboard functionality testing

**3. Data Quality Testing**:
- Input validation testing
- Output verification
- Edge case handling

### **Test Data Requirements**

**1. Valid Test Cases**:
- Properly formatted CSV and XML files
- Valid mobile numbers and order IDs
- Complete data records

**2. Invalid Test Cases**:
- Malformed mobile numbers
- Invalid order ID formats
- Missing required fields
- Negative amounts/quantities

**3. Edge Cases**:
- Empty files
- Large datasets
- Special characters in names
- Boundary value testing

---

## ğŸš€ Deployment Guide

### **Environment Setup**

**1. System Requirements**:
```bash
# Python 3.8+
python --version

# MySQL 8.0+
mysql --version

# Required Python packages
pip install -r requirements.txt
```

**2. Database Setup**:
```sql
-- Create database
CREATE DATABASE akasa_db;

-- Create user (optional)
CREATE USER 'etl_user'@'localhost' IDENTIFIED BY 'secure_password';
GRANT ALL PRIVILEGES ON akasa_db.* TO 'etl_user'@'localhost';
```

**3. Environment Configuration**:
```env
# .env file
MYSQL_USER=etl_user
MYSQL_PASSWORD=secure_password
MYSQL_DB=akasa_db
MYSQL_HOST=localhost
```

### **Deployment Options**

**1. Local Development**:
```bash
# Clone repository
git clone <repository-url>
cd akasa-air-etl

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your database credentials

# Run pipeline
python main.py --customers data.csv --orders data.xml --dashboard
```

**2. Production Deployment**:
```bash
# Use virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows

# Install production dependencies
pip install -r requirements.txt

# Set production environment variables
export MYSQL_USER=prod_user
export MYSQL_PASSWORD=secure_prod_password
export MYSQL_DB=akasa_prod_db
export MYSQL_HOST=prod-db-server

# Run in production mode
python main.py --customers /path/to/customers.csv --orders /path/to/orders.xml
```

**3. Docker Deployment** (Future Enhancement):
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

### **Monitoring & Maintenance**

**1. Log Monitoring**:
- Monitor application logs for errors
- Set up log rotation for long-running processes
- Alert on critical errors

**2. Database Monitoring**:
- Monitor database performance
- Check for data quality issues
- Regular backup procedures

**3. Dashboard Monitoring**:
- Monitor Streamlit process health
- Check dashboard accessibility
- Monitor resource usage

---

## ğŸ“Š Business Intelligence Features

### **Key Performance Indicators (KPIs)**

**1. Customer Metrics**:
- Total unique customers
- Repeat customer identification
- Customer acquisition trends
- Regional customer distribution

**2. Order Metrics**:
- Total order volume
- Average order value
- Order frequency patterns
- Monthly growth rates

**3. Revenue Metrics**:
- Total revenue
- Revenue by region
- Revenue trends over time
- High-value order analysis

### **Analytics Capabilities**

**1. Trend Analysis**:
- Month-over-month growth
- Seasonal patterns
- Performance trajectories

**2. Segmentation Analysis**:
- Customer value segmentation
- Regional performance comparison
- Order value distribution

**3. Operational Insights**:
- Processing efficiency metrics
- Data quality indicators
- System performance monitoring

---

## ğŸ”„ Future Enhancements

### **Planned Improvements**

**1. Advanced Analytics**:
- Customer lifetime value calculation
- Predictive analytics for customer behavior
- Advanced segmentation algorithms

**2. Real-time Processing**:
- Stream processing capabilities
- Real-time dashboard updates
- Event-driven architecture

**3. Enhanced Visualization**:
- Advanced chart types
- Interactive drill-down capabilities
- Export functionality

**4. Scalability Improvements**:
- Distributed processing
- Cloud deployment options
- Auto-scaling capabilities

### **Technical Debt & Optimization**

**1. Code Improvements**:
- Comprehensive unit test suite
- Performance profiling and optimization
- Code coverage analysis

**2. Infrastructure**:
- Container orchestration
- CI/CD pipeline implementation
- Automated deployment processes

**3. Monitoring**:
- Application performance monitoring
- Business metrics tracking
- Automated alerting systems

---

## ğŸ“ Support & Troubleshooting

### **Common Issues & Solutions**

**1. Database Connection Issues**:
```bash
# Check MySQL service
sudo systemctl status mysql

# Test connection
mysql -u username -p -h hostname database_name

# Verify environment variables
echo $MYSQL_USER
```

**2. Data Processing Errors**:
- Verify file formats match expected schema
- Check for special characters in data
- Validate file permissions and accessibility

**3. Dashboard Issues**:
```bash
# Check Streamlit installation
streamlit --version

# Test dashboard directly
streamlit run dashboard_app.py

# Check port availability
netstat -tulpn | grep 8501
```

### **Performance Troubleshooting**

**1. Slow Database Operations**:
- Check database indexes
- Monitor connection pool usage
- Analyze query execution plans

**2. Memory Issues**:
- Monitor memory usage during processing
- Consider processing data in smaller chunks
- Optimize DataFrame operations

**3. Dashboard Performance**:
- Check query execution times
- Monitor browser console for errors
- Optimize visualization rendering

---

## ğŸ“‹ Conclusion

This ETL pipeline solution provides a comprehensive, production-ready system for processing Akasa Air's customer and order data. The solution exceeds typical ETL requirements by including:

- **Enterprise-grade architecture** with proper error handling and security
- **Interactive analytics dashboard** for immediate business insights
- **Scalable design** that can handle growing data volumes
- **Comprehensive documentation** for maintenance and enhancement

The modular design ensures easy maintenance and future enhancements, while the integrated dashboard provides immediate business value through actionable insights and real-time analytics.

**Key Success Metrics**:
- âœ… 100% automated data processing
- âœ… Comprehensive data validation and quality assurance
- âœ… Real-time business intelligence capabilities
- âœ… Production-ready deployment with proper documentation
- âœ… Scalable architecture for future growth

This solution transforms raw data into actionable business intelligence, providing Akasa Air with the tools needed for data-driven decision making and operational excellence.